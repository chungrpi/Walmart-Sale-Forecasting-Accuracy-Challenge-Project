{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/practice coding/kaggle/M5 Forecasting - Accuracy/m5-forecasting-accuracy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to reduce memory used\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data files\n",
    "sales_train_validation = pd.read_csv(path + 'sales_train_validation.csv')\n",
    "sell_price = pd.read_csv(path + 'sell_prices.csv')\n",
    "calendar = pd.read_csv(path + 'calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sales_train_valdation table to have a day column and a demand column\n",
    "sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "#convert day column from string to number of days in integer\n",
    "sales_train_validation['day'] = sales_train_validation['day'].apply(lambda x: int(x[2:])).astype('int16')\n",
    "#Because data is too large, days larger or equal to 1434 are only selected for the analysis\n",
    "sales_train_validation = sales_train_validation.loc[sales_train_validation.day >= 1434] #day 1434 is 01/01/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Canlendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert day column from string to number of days in integer\n",
    "calendar['d'] = calendar['d'].apply(lambda x: int(x[2:])).astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add more day and time features\n",
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the add_datepart function already has Dayofweek column, weekday column of calendar will be dropped to save memory\n",
    "calendar.drop(['weekday'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add day and time features to calendar\n",
    "add_datepart(calendar, 'date', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add season feature\n",
    "def to_season(date):    \n",
    "    #convert date to season for the northen hemisphere\n",
    "    #\"day of year\" ranges for the northern hemisphere\n",
    "    spring = range(80, 172)\n",
    "    summer = range(172, 264)\n",
    "    fall = range(264, 355)\n",
    "    # winter = everything else\n",
    "    doy = date.timetuple().tm_yday\n",
    "    if doy in spring:\n",
    "      season = 'Spring'\n",
    "    elif doy in summer:\n",
    "      season = 'Summer'\n",
    "    elif doy in fall:\n",
    "      season = 'Fall'\n",
    "    else:\n",
    "      season = 'Winter'\n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add season feature to calendar\n",
    "calendar['Season'] = calendar.date.transform(lambda x: to_season(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Calendar Data and Sell Price Data with Sales Train Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_validation = sales_train_validation.merge(calendar, how='left', left_on='day', right_on='d')\n",
    "sales_train_validation.drop('d', axis=1, inplace=True)\n",
    "sales_train_validation = sales_train_validation.merge(sell_price, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. PREPARE FOR PREDICTION DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the lag features and rolling window will be created for the analysis. The prediction data will be added to the training data to make lag features and rolling window for them too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the prediction data\n",
    "sample_submission = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "#Because the sale train validation data was melted so it was changed. To get the columns needed for prediction data\n",
    "#the original sales train validation data is read again\n",
    "sales_train_validation1 = pd.read_csv(path + 'sales_train_validation.csv')\n",
    "sales_train_validation1 = sales_train_validation1[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n",
    "\n",
    "#Only take the validation rows which is 28 days after the trainning data day for the prediction\n",
    "validation = sample_submission.loc[sample_submission.id.str.contains('validation')]\n",
    "#Get id, item_id, dept_id, cat_id, store_id, state_id for the validation data for prediction\n",
    "validation = validation.merge(sales_train_validation1, how='left', on='id')\n",
    "\n",
    "validation = pd.melt(validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "validation['day'] = validation['day'].apply(lambda x: int(x[1:])).astype('int16')\n",
    "#The training data day is end at 1913, so the day column of validation is added to 1913 to consider the days after\n",
    "#the trainning data day\n",
    "validation['day'] = validation['day'] + 1913\n",
    "validation = validation.merge(calendar, how='left', left_on='day', right_on='d')\n",
    "validation.drop('d', axis=1, inplace=True)\n",
    "validation = validation.merge(sell_price, how='left', on=['item_id', 'store_id', 'wm_yr_wk',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. PREPARING FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((sales_train_validation, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fill missing data for sell price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_price(data):\n",
    "  mean_sell_price = data.loc[data.sell_price.isnull()==False][['item_id','sell_price']].groupby('item_id', as_index=False).mean()\n",
    "  mean_sell_price_per_item = {}\n",
    "  for i in range(mean_sell_price.shape[0]):\n",
    "    mean_sell_price_per_item[mean_sell_price['item_id'].iloc[i]] = mean_sell_price['sell_price'].iloc[i]\n",
    "    \n",
    "  sell_price_nan = data['sell_price'].isnull().values\n",
    "  sell_price_nan_loc = np.where(sell_price_nan == True)[0]\n",
    "\n",
    "  for i in sell_price_nan_loc:\n",
    "    id_ = data['item_id'].iloc[i]\n",
    "    data['sell_price'].iloc[i] = mean_sell_price_per_item[id_]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fillna_price(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fe(data):\n",
    "    \n",
    "    shift_window_size = [28]\n",
    "    lag_day = [1,2,3,4,5,6,7]\n",
    "    roll_demand_days = [7,14,30,60,180]\n",
    "    \n",
    "    #Add lag demand feature     \n",
    "    for i in shift_window_size:\n",
    "        for j in lag_day:\n",
    "            data['lag_pos'+str(j)] = data.groupby('item_id', as_index=False)['demand'].transform(lambda x: x.shift(i+j))\n",
    "       \n",
    "\n",
    "    ### Moving average on item_id         \n",
    "    for i in shift_window_size:\n",
    "      for j in roll_demand_days:\n",
    "        data['demand_roll_mean_'+str(j)] = data.groupby('item_id', as_index=False)['demand'].transform(lambda x: x.shift(i).rolling(window=j, min_periods=1).mean())\n",
    "        data['demand_roll_std_'+str(j)] = data.groupby('item_id', as_index=False)['demand'].transform(lambda x: x.shift(i).rolling(window=j, min_periods=1).std())        \n",
    "        data['demand_roll_q25_'+str(j)] = data.groupby('item_id', as_index=False)['demand'].transform(lambda x: x.shift(i).rolling(window=j, min_periods=1).quantile(0.25))        \n",
    "        data['demand_roll_q75_'+str(j)] = data.groupby('item_id', as_index=False)['demand'].transform(lambda x: x.shift(i).rolling(window=j, min_periods=1).quantile(0.75))        \n",
    "        data['demand_roll_IQR_'+str(j)] = data['demand_roll_q75_'+str(j)] - data['demand_roll_q25_'+str(j)]    \n",
    "        \n",
    "    \n",
    "    ### Moving average on item_id and dept_id\n",
    "    for i in shift_window_size:\n",
    "       for j in roll_demand_days:\n",
    "         data['demand_dept_roll_mean_'+str(j)] = data.groupby(['item_id','dept_id'], as_index=False)['demand'].transform(lambda x: x.shift(i).rolling(j).mean())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_fe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce memory used by data\n",
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. XGBOOST MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[data.day <= 1913].drop(['id','demand','date'], axis=1)\n",
    "y = data.loc[data.day <= 1913]['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb(n_estimators=1000, learning_rate=0.03, n_jobs=-1, colsample_bytree=1.0,\n",
    "                eta=0.01, max_depth=10, min_child_weight=6, objective='reg:linear',gamma=0.3,tree_method='exact')\n",
    "xgb_model.fit(X, y, eval_set=[(X,y)], early_stopping_rounds=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. MAKE PREDICTION FOR 28 DAYS AFTER DAY 1913**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_demand_pred = xgb_model.predict(data.loc[data.day > 1913].drop(['id','demand','date'], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
